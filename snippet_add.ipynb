{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb483c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "from keras.layers import Flatten, Input, Dense, Lambda\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "from keras.preprocessing import image\n",
    "\n",
    "from keras.models import Sequential, Model\n",
    "from glob import glob\n",
    "import cv2\n",
    "from sklearn import model_selection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dcc339f",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices(\"GPU\")\n",
    "for gpu in gpus:\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "352d907d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a vector of images with label 1 when both resembles same image and 0 when both resembles different image\n",
    "\n",
    "imagev=[]\n",
    "files= sorted(os.listdir(\"downloaded_images\"))\n",
    "i=1;\n",
    "for file in files:\n",
    "    if file.endswith(('.png', '.jpg', '.jpeg', '.bmp')):\n",
    "        \n",
    "        imgloc = os.path.join(\"downloaded_images\", file)\n",
    "        img = Image.open(imgloc)\n",
    "        imagev.append((img, file))\n",
    "# #         plt.figure(figsize=(80,5))\n",
    "# #         plt.subplot(1,16,i)\n",
    "# #         plt.imshow(img)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe08dc38",
   "metadata": {},
   "outputs": [],
   "source": [
    "match=[]\n",
    "unmatch=[]\n",
    "k=0\n",
    "\n",
    "for i in range(len(imagev)):\n",
    "    \n",
    "    for j in range(len(imagev)):\n",
    "        k=k+1\n",
    "        if(i>0 and j>i):\n",
    "            if(imagev[i][1][:10]==imagev[j][1][:10]):\n",
    "                match.append((imagev[i][0],imagev[j][0]))\n",
    "            elif(len(imagev[i][1])>10 and len(imagev[j][1])>10 and k%300==0) :\n",
    "                unmatch.append((imagev[i][0],imagev[j][0]))\n",
    "    \n",
    "            \n",
    "    \n",
    "\n",
    "# for x,y in unmatch:\n",
    "#     plt.subplot(1,2,1)\n",
    "#     plt.imshow(x)\n",
    "#     plt.subplot(1,2,2)\n",
    "#     plt.imshow(y)\n",
    "#     plt.show()\n",
    "len(unmatch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "582125b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "face_cascade = cv2.CascadeClassifier(\n",
    "    cv2.data.haarcascades + \"haarcascade_frontalface_default.xml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8963e3d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#      # Convert PIL images to numpy arrays\n",
    "#     x2 = np.array(x1)\n",
    "#     y2 = np.array(y1)\n",
    "    \n",
    "#     # Convert images to grayscale for face detection\n",
    "#     x3 = cv2.cvtColor(x2, cv2.COLOR_BGR2GRAY)\n",
    "#     y3 = cv2.cvtColor(y2, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "#     # Detect faces in x3\n",
    "#     faces_x = face_cascade.detectMultiScale(x3, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))\n",
    "#     for (a, b, w, h) in faces_x:\n",
    "#         x4 = x2[b:b + h, a:a + w]  # Crop the face region\n",
    "#         break  # Assuming only the first detected face is needed\n",
    "\n",
    "#     # Detect faces in y3\n",
    "#     faces_y = face_cascade.detectMultiScale(y3, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))\n",
    "#     for (a, b, w, h) in faces_y:\n",
    "#         y4 = y2[b:b + h, a:a + w]  # Crop the face region\n",
    "#         break  # Assuming only the first detected face is needed\n",
    "\n",
    "#     # Resize the cropped face regions to (224, 224)\n",
    "#     x5 = cv2.resize(x4, (224, 224))\n",
    "#     y5 = cv2.resize(y4, (224, 224))\n",
    "\n",
    "    \n",
    "#     # Convert PIL images to numpy arrays\n",
    "#     x2 = np.array(x1)\n",
    "#     y2 = np.array(y1)\n",
    "    \n",
    "#     # Convert images to grayscale for face detection\n",
    "#     x3 = cv2.cvtColor(x2, cv2.COLOR_BGR2GRAY)\n",
    "#     y3 = cv2.cvtColor(y2, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "#     # Detect faces in x3\n",
    "#     faces_x = face_cascade.detectMultiScale(x3, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))\n",
    "#     for (a, b, w, h) in faces_x:\n",
    "#         x4 = x2[b:b + h, a:a + w]  # Crop the face region\n",
    "#         break  # Assuming only the first detected face is needed\n",
    "\n",
    "#     # Detect faces in y3\n",
    "#     faces_y = face_cascade.detectMultiScale(y3, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))\n",
    "#     for (a, b, w, h) in faces_y:\n",
    "#         y4 = y2[b:b + h, a:a + w]  # Crop the face region\n",
    "#         break  # Assuming only the first detected face is needed\n",
    "\n",
    "#     # Resize the cropped face regions to (224, 224)\n",
    "#     x5 = cv2.resize(x4, (224, 224))\n",
    "#     y5 = cv2.resize(y4, (224, 224))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "285bb19f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(match[4][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "918813ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mtcnn import MTCNN\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "detector = MTCNN()\n",
    "\n",
    "X = []\n",
    "Y = []\n",
    "\n",
    "for x, y in match:\n",
    "    # Convert PIL images to numpy arrays\n",
    "    x2 = np.array(x)\n",
    "    y2 = np.array(y)\n",
    "\n",
    "    # Convert images to grayscale for face detection\n",
    "    x3 = cv2.cvtColor(x2, cv2.COLOR_BGR2GRAY)\n",
    "    y3 = cv2.cvtColor(y2, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Convert grayscale images back to RGB\n",
    "    x_rgb = cv2.cvtColor(x3, cv2.COLOR_GRAY2RGB)\n",
    "    y_rgb = cv2.cvtColor(y3, cv2.COLOR_GRAY2RGB)\n",
    "\n",
    "    # Detect faces in x_rgb\n",
    "    faces_x = detector.detect_faces(x_rgb)\n",
    "    if faces_x:\n",
    "        x4 = x2[faces_x[0]['box'][1]:faces_x[0]['box'][1]+faces_x[0]['box'][3],\n",
    "               faces_x[0]['box'][0]:faces_x[0]['box'][0]+faces_x[0]['box'][2]]  # Crop the face region\n",
    "        x5 = cv2.resize(x4, (224, 224))  # Resize the cropped face region\n",
    "    else:\n",
    "        continue  # Skip if no face detected\n",
    "\n",
    "    # Detect faces in y_rgb\n",
    "    faces_y = detector.detect_faces(y_rgb)\n",
    "    if faces_y:\n",
    "        y4 = y2[faces_y[0]['box'][1]:faces_y[0]['box'][1]+faces_y[0]['box'][3],\n",
    "               faces_y[0]['box'][0]:faces_y[0]['box'][0]+faces_y[0]['box'][2]]  # Crop the face region\n",
    "        y5 = cv2.resize(y4, (224, 224))  # Resize the cropped face region\n",
    "    else:\n",
    "        continue  # Skip if no face detected\n",
    "\n",
    "    # Append to lists\n",
    "    X.append([x5, y5])\n",
    "    Y.append(1)\n",
    "\n",
    "for x, y in unmatch:\n",
    "    # Convert PIL images to numpy arrays\n",
    "    x2 = np.array(x)\n",
    "    y2 = np.array(y)\n",
    "\n",
    "    # Convert images to grayscale for face detection\n",
    "    x3 = cv2.cvtColor(x2, cv2.COLOR_BGR2GRAY)\n",
    "    y3 = cv2.cvtColor(y2, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Convert grayscale images back to RGB\n",
    "    x_rgb = cv2.cvtColor(x3, cv2.COLOR_GRAY2RGB)\n",
    "    y_rgb = cv2.cvtColor(y3, cv2.COLOR_GRAY2RGB)\n",
    "\n",
    "    # Detect faces in x_rgb\n",
    "    faces_x = detector.detect_faces(x_rgb)\n",
    "    if faces_x:\n",
    "        x4 = x2[faces_x[0]['box'][1]:faces_x[0]['box'][1]+faces_x[0]['box'][3],\n",
    "               faces_x[0]['box'][0]:faces_x[0]['box'][0]+faces_x[0]['box'][2]]  # Crop the face region\n",
    "        x5 = cv2.resize(x4, (224, 224))  # Resize the cropped face region\n",
    "    else:\n",
    "        continue  # Skip if no face detected\n",
    "\n",
    "    # Detect faces in y_rgb\n",
    "    faces_y = detector.detect_faces(y_rgb)\n",
    "    if faces_y:\n",
    "        y4 = y2[faces_y[0]['box'][1]:faces_y[0]['box'][1]+faces_y[0]['box'][3],\n",
    "               faces_y[0]['box'][0]:faces_y[0]['box'][0]+faces_y[0]['box'][2]]  # Crop the face region\n",
    "        y5 = cv2.resize(y4, (224, 224))  # Resize the cropped face region\n",
    "    else:\n",
    "        continue  # Skip if no face detected\n",
    "\n",
    "    # Append to lists\n",
    "    X.append([x5, y5])\n",
    "    Y.append(-1)\n",
    "\n",
    "# Convert lists to numpy arrays\n",
    "X_me = np.array(X)\n",
    "Y_me = np.array(Y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28582b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# # Initialize face detector\n",
    "# face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "\n",
    "# X = []\n",
    "# Y = []\n",
    "\n",
    "# for x, y in match:\n",
    "   \n",
    "    \n",
    "#     # Resize PIL images\n",
    "#     x1 = x.resize((224, 224))\n",
    "#     y1 = y.resize((224, 224))\n",
    "    \n",
    "#     # Convert PIL images to numpy arrays\n",
    "#     x2 = np.array(x1)\n",
    "#     y2 = np.array(y1)\n",
    "    \n",
    "#     # Convert images to grayscale for face detection\n",
    "#     x3 = cv2.cvtColor(x2, cv2.COLOR_BGR2GRAY)\n",
    "#     y3 = cv2.cvtColor(y2, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "#     # Detect faces in x3\n",
    "#     faces_x = face_cascade.detectMultiScale(x3, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))\n",
    "#     for (a, b, w, h) in faces_x:\n",
    "#         x4 = x2[b:b + h, a:a + w]  # Crop the face region\n",
    "#         break  # Assuming only the first detected face is needed\n",
    "\n",
    "#     # Detect faces in y3\n",
    "#     faces_y = face_cascade.detectMultiScale(y3, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))\n",
    "#     for (a, b, w, h) in faces_y:\n",
    "#         y4 = y2[b:b + h, a:a + w]  # Crop the face region\n",
    "#         break  # Assuming only the first detected face is needed\n",
    "\n",
    "#     # Resize the cropped face regions to (224, 224)\n",
    "#     x5 = cv2.resize(x4, (224, 224))\n",
    "#     y5 = cv2.resize(y4, (224, 224))\n",
    "\n",
    "#     # Append to lists\n",
    "#     X.append([x5, y5])\n",
    "#     Y.append(1)\n",
    "\n",
    "# for x, y in unmatch:\n",
    "#     # Resize PIL images\n",
    "#     x1 = x.resize((224, 224))\n",
    "#     y1 = y.resize((224, 224))\n",
    "#      # Convert PIL images to numpy arrays\n",
    "#     x2 = np.array(x1)\n",
    "#     y2 = np.array(y1)\n",
    "    \n",
    "#     # Convert images to grayscale for face detection\n",
    "#     x3 = cv2.cvtColor(x2, cv2.COLOR_BGR2GRAY)\n",
    "#     y3 = cv2.cvtColor(y2, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "#     # Detect faces in x3\n",
    "#     faces_x = face_cascade.detectMultiScale(x3, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))\n",
    "#     for (a, b, w, h) in faces_x:\n",
    "#         x4 = x2[b:b + h, a:a + w]  # Crop the face region\n",
    "#         break  # Assuming only the first detected face is needed\n",
    "\n",
    "#     # Detect faces in y3\n",
    "#     faces_y = face_cascade.detectMultiScale(y3, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))\n",
    "#     for (a, b, w, h) in faces_y:\n",
    "#         y4 = y2[b:b + h, a:a + w]  # Crop the face region\n",
    "#         break  # Assuming only the first detected face is needed\n",
    "\n",
    "#     # Resize the cropped face regions to (224, 224)\n",
    "#     x5 = cv2.resize(x4, (224, 224))\n",
    "#     y5 = cv2.resize(y4, (224, 224))\n",
    "\n",
    "#     # Append to lists\n",
    "#     X.append([x5, y5])\n",
    "    \n",
    "#     Y.append(-1)\n",
    "\n",
    "# # Convert lists to numpy arrays\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fbdfbff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert lists to numpy arrays\n",
    "X_me = np.array(X)\n",
    "Y_me = np.array(Y)\n",
    "X_me.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2efc7eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,Y_train,Y_test=model_selection.train_test_split(X_me,Y_me,test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf267268",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11426ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(unmatch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24af8d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "## making triplet for triplet loss layer\n",
    "# k=0\n",
    "# triplet=[]\n",
    "# for i in range(len(imagev)):\n",
    "#     temp=[]\n",
    "#     temp.append((imagev[i][0]))\n",
    "#     for j in range(len(imagev)):\n",
    "#         k=k+1\n",
    "#         if(i>0 and j>i):\n",
    "#             if(imagev[i][1][:10]==imagev[j][1][:10] and len(temp)==1):\n",
    "#                 temp.append((imagev[j][0]))\n",
    "#             if(imagev[i][1][:10]!=imagev[j][1][:10] and len(temp)==2) :\n",
    "#                 temp.append((imagev[j][0]))\n",
    "#                 triplet.append(temp)\n",
    "#                 break\n",
    "    \n",
    "            \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9ac174e",
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "\n",
    "# for x,y,z in triplet:\n",
    "#     plt.subplot(1,3,1)\n",
    "#     plt.imshow(x)\n",
    "#     plt.subplot(1,3,2)\n",
    "#     plt.imshow(y)\n",
    "#     plt.subplot(1,3,3)\n",
    "#     plt.imshow(z)\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "320a882a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# anc_in=[]\n",
    "# pos_in=[]\n",
    "# neg_in=[]\n",
    "# target=(224,224)\n",
    "# for x,y,z in triplet:\n",
    "#     anc_resize=x.resize(target)\n",
    "#     anc_in.append(anc_resize)\n",
    "#     pos_resize=y.resize(target)\n",
    "#     pos_in.append(pos_resize)\n",
    "#     neg_resize=z.resize(target)\n",
    "#     neg_in.append(neg_resize)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03a7151e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(triplet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0225d317",
   "metadata": {},
   "outputs": [],
   "source": [
    "Img_shape=[224,224,3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97957cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path=\"g.jpg\"\n",
    "img=Image.open(img_path)\n",
    "img_ar=np.array(img)\n",
    "a=img.resize((224,224))\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9224e4b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg=VGG16(input_shape=(224,224,3),weights='imagenet',include_top=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bd24cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in vgg.layers:\n",
    "    layer.trainable=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "639cc216",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def triplet_loss(y_true,y_pred,alpha=0.5):\n",
    "#     anc_o,pos_o,neg_o=y_pred[0],y_pred[1],y_pred[2]\n",
    "#     pos=tf.reduce_mean(tf.square(anc_o-pos_o))\n",
    "#     neg=tf.reduce_mean(tf.square(anc_o-neg_o))\n",
    "#     loss1=pos-neg\n",
    "#     loss=tf.maximum(loss1+alpha,0)\n",
    "    \n",
    "#     return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d3faa5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity_loss(y_true,y_pred):\n",
    "    loss=tf.abs(y_true-y_pred)\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ab9883",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_shape=(224,224,3)\n",
    " \n",
    "# anchor_input=Input(shape=input_shape,name='input for anchor')\n",
    "# positive_input=Input(shape=input_shape,name='input for positive image')\n",
    "# negative_input=Input(shape=input_shape,name='input for negative image')\n",
    "\n",
    "# anchor_output_vgg=Flatten()(vgg(anchor_input))\n",
    "# positive_output_vgg=Flatten()(vgg(positive_input))\n",
    "# negative_output_vgg=Flatten()(vgg(negative_input))\n",
    "\n",
    "# last_dense=Dense(128,activation='relu')\n",
    "# anc_op=last_dense(anchor_output_vgg)\n",
    "# pos_op=last_dense(positive_output_vgg)\n",
    "# neg_op=last_dense(negative_output_vgg)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ff52705",
   "metadata": {},
   "outputs": [],
   "source": [
    "# one more thing you need to add triplet loss function bro\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ded31edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Example input shape (replace with your actual input shape)\n",
    "input_shape = (224, 224, 3)\n",
    "\n",
    "# Example input layers\n",
    "one_input = Input(shape=input_shape, name='input_one')\n",
    "two_input = Input(shape=input_shape, name='input_two')\n",
    "\n",
    "\n",
    "# Example outputs from VGG16\n",
    "output_vgg_one = Flatten()(vgg(one_input))\n",
    "output_vgg_two = Flatten()(vgg(two_input))\n",
    "\n",
    "# Example dense layers\n",
    "lt_dense = Dense(128)\n",
    "\n",
    "# Applying dense layers\n",
    "first_in = lt_dense(output_vgg_one)\n",
    "second_in = lt_dense(output_vgg_two)\n",
    "\n",
    "# Custom similarity metric wrapped in Lambda layer\n",
    "def similarity_metric(inputs):\n",
    "    first_in, second_in = inputs\n",
    "    dot_product = tf.reduce_sum(first_in * second_in, axis=-1)\n",
    "    norm_first = tf.norm(first_in, axis=-1)\n",
    "    norm_second = tf.norm(second_in, axis=-1)\n",
    "    similarity = dot_product / (norm_first * norm_second)\n",
    "    return similarity\n",
    "\n",
    "out_before_loss = Lambda(similarity_metric, name='similarity_metric')([first_in, second_in])\n",
    "\n",
    "# Construct the model\n",
    "model_c = Model(inputs=[one_input, two_input], outputs=out_before_loss)\n",
    "\n",
    "# Print model summary for verification\n",
    "model_c.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bfc6f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# def my_accuracy(y_true, y_pred):\n",
    "#     # Cast y_true to float32 if necessary (assuming y_true is int64)\n",
    "#     y_true = tf.cast(y_true, tf.float32)\n",
    "    \n",
    "#     # Check if signs of y_true and y_pred are the same\n",
    "# #     correct_predictions = tf.equal(tf.sign(y_true), tf.sign(y_pred))\n",
    "#     total=0.0\n",
    "#     error=0.0\n",
    "# #     y_true=np.array(y_true)\n",
    "# #     y_pred=np.array(y_pred)\n",
    "#     for a,b in (y_true,y_pred):\n",
    "#         if(tf.abs(a-b)>=1):\n",
    "#             error=error+1\n",
    "#         total=total+1\n",
    "#     return (1.0-error/total)\n",
    "            \n",
    "    \n",
    "#     # Cast boolean to float and take the mean\n",
    "#     return tf.reduce_mean(tf.cast(correct_predictions, tf.float32))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa60ec68",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def my_accuracy(y_true, y_pred):\n",
    "    y_true=tf.cast(y_true,tf.float32)\n",
    "    for i in range(20):\n",
    "        if(tf.abs(y_true[i]-y_pred[i])>=1):\n",
    "            error=error+1\n",
    "    return error/20\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c408832a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_c.compile(optimizer='adam',loss='mse')\n",
    "model_c.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ddbfb7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train1=X_train[:,0]\n",
    "X_train1.shape\n",
    "X_train2=X_train[:,1]\n",
    "Y_train=Y_train.reshape(-1,1)\n",
    "X_test1=X_test[:,0]\n",
    "X_test2=X_test[:,1]\n",
    "Y_test=Y_test.reshape(-1,1)\n",
    "Y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1a6b7f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for x,y,z in zip(X_test1,X_test2,Y_test):\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.imshow(x)\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.imshow(y)\n",
    "    print(z)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "737227af",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_cosine=([X_train1,X_train2])\n",
    "test_cosine=([X_test1,X_test2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "964f35e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a8ab24f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_c.fit(input_cosine,Y_train,epochs=20,batch_size=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "325621ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "c=model_c.predict(test_cosine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ad6d531",
   "metadata": {},
   "outputs": [],
   "source": [
    "error=0\n",
    "total=0;\n",
    "for i in range(len(Y_test)):\n",
    "    if(tf.abs(c[i]-Y_test[i])>=1):\n",
    "        error=error+1\n",
    "    total=total+1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03e5b9d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(1-error/total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ce69ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_accuracy(Y_test,c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6088dfaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bae54b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=Model(inputs=[anchor_input,positive_input,negative_input],outputs=[anc_op,pos_op,neg_op])\n",
    "model.compile(optimizer='adam',loss= triplet_loss)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a0e2d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "anc_in_ar=np.array(anc_in)\n",
    "pos_in_ar=np.array(pos_in)\n",
    "neg_in_ar=np.array(neg_in)\n",
    "inputs=([anc_in_ar,pos_in_ar,neg_in_ar])\n",
    "dummy_targets = np.zeros((369,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8134d70d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(inputs,dummy_targets,epochs=100,batch_size=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d76d9811",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ae49916",
   "metadata": {},
   "outputs": [],
   "source": [
    "anc1=anc_in_ar[0].reshape(1,224,224,3)\n",
    "anc1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73a14f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "last_dense(Flatten()(vgg(anc1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38231f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "inp=np.array(inputs)\n",
    "inp1=inp[:,1,:,:,:].reshape(3,1,224,224,3)\n",
    "inp2=inp1.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6d41c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict(inp1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "593cf9dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ar=np.array(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdb7ec52",
   "metadata": {},
   "outputs": [],
   "source": [
    "image=Image.fromarray(input_ar[0][0])\n",
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9459c6a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7bb806d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "224b37d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e213a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0b60f73",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ff4e2c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
